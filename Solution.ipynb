{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>DATE</th>\n",
       "      <th>KT_RESULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>301478</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>КТ-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18024</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>КТ-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>191578</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>КТ-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215212</td>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>КТ-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>689401</td>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>КТ-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      idx        DATE KT_RESULT\n",
       "0  301478  2020-04-17      КТ-1\n",
       "1   18024  2020-04-17      КТ-0\n",
       "2  191578  2020-04-17      КТ-1\n",
       "3  215212  2020-04-18      КТ-1\n",
       "4  689401  2020-04-18      КТ-2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "\n",
    "df_r = pd.read_csv(\"data_for_science_KT_train.csv\", encoding='utf-8', sep=',')\n",
    "df_r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = df_r.sort_values(\"DATE\", kind=\"stable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>DATE</th>\n",
       "      <th>KT_RESULT</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>301478</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>КТ-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18024</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>КТ-0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>191578</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>КТ-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39305</th>\n",
       "      <td>623270</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>КТ-2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39306</th>\n",
       "      <td>327426</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>КТ-0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          idx        DATE KT_RESULT  y\n",
       "0      301478  2020-04-17      КТ-1  1\n",
       "1       18024  2020-04-17      КТ-0  0\n",
       "2      191578  2020-04-17      КТ-1  1\n",
       "39305  623270  2020-04-17      КТ-2  2\n",
       "39306  327426  2020-04-17      КТ-0  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r[\"y\"] = df_r.KT_RESULT.apply(lambda x: int(x.split('-')[1]))\n",
    "df_r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = pd.read_csv(\"data_for_science_2020-07-27_w_mu_type.csv\", encoding='utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDate(str_date):\n",
    "    if(isinstance(str_date, str)==False):\n",
    "        return 0\n",
    "    \n",
    "    s = str_date.split('-')\n",
    "    day = s[2]\n",
    "    month = s[1]\n",
    "    year=s[0]\n",
    "    \n",
    "    list_of_month = [\"01\", \"02\", \"03\", \"04\", \"05\",\"06\",\"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "    month = list_of_month.index(month) + 1\n",
    "    \n",
    "    day = int(day[1])  if(day[0]=='0') else int(day)\n",
    "    \n",
    "    year = int(year)\n",
    "    \n",
    "    st = time.struct_time(\n",
    "    (year, month, day, 0, 0, 0, 0, 0, 0)\n",
    "    )\n",
    "    \n",
    "    return time.mktime(st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseYear(str_date):\n",
    "    if(isinstance(str_date, str)==False):\n",
    "        return 0\n",
    "    \n",
    "    year = str_date.split('-')[0]\n",
    "    year = int(year)\n",
    "    \n",
    "    return year \n",
    "\n",
    "\n",
    "class SimpleYearExtractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table):\n",
    "    \n",
    "    birth_dates = table[[\"idx\", \"birth_dt\"]].groupby(\"idx\").min().reset_index()\n",
    "    birth_dates[\"year\"] = birth_dates.birth_dt.apply(parseYear)\n",
    "    self.dates = birth_dates[[\"idx\", \"year\"]].copy()\n",
    "    \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    \n",
    "    return X.merge(self.dates, how='left', on='idx')[[\"year\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleYearExtractor(df_a).fit_transform(df_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCountExtractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "    \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    rez = X[[\"idx\", \"DATE\"]].copy()\n",
    "\n",
    "    groupby_object = X[[\"idx\"]].groupby('idx')\n",
    "    #how many ct patient had\n",
    "    rez[\"count\"] = groupby_object['idx'].transform(len)\n",
    "    #CT's number\n",
    "    rez[\"cum_count\"] = groupby_object.cumcount()+1\n",
    "    #CT's reverse number\n",
    "    rez[\"count_ost\"] = rez[\"count\"].values - rez[\"cum_count\"].values\n",
    "\n",
    "    \n",
    "    return rez[[\"cum_count\", \"count\", 'count_ost']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTimeExtractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "    pass\n",
    "    \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    rez = X.copy()\n",
    "\n",
    "    rez[\"time\"] = rez.DATE.apply(parseDate)\n",
    "    groupby_object = rez.groupby('idx')\n",
    "    \n",
    "    #how many СT patient had\n",
    "    rez[\"count\"] = groupby_object['idx'].transform(len)\n",
    "    #time to next CT\n",
    "    rez[\"prev_time_dif\"] = groupby_object['time'].diff(1).fillna(-1)\n",
    "    #Time to prev CT\n",
    "    rez[\"next_time_dif\"] = groupby_object['time'].diff(-1).fillna(1)\n",
    "    \n",
    "    #count statistics only where it make sense\n",
    "    rez_filtered = rez[(rez[\"count\"] > 1)&(rez[\"prev_time_dif\"] != -1)]#.copy()\n",
    "    \n",
    "    #mean and std time differnces between patient's CTs\n",
    "    patient_time_stats = rez_filtered[[\"idx\", \"prev_time_dif\"]].groupby('idx').agg(\n",
    "         time_mean = pd.NamedAgg(column=\"prev_time_dif\", aggfunc=np.mean),\n",
    "         time_std = pd.NamedAgg(column=\"prev_time_dif\", aggfunc=np.std)).reset_index()\n",
    "\n",
    "    rez = rez.merge(patient_time_stats, on=\"idx\", how=\"left\").fillna(-1)\n",
    "\n",
    "    return rez[[\"idx\", \"time\", \"prev_time_dif\", \"next_time_dif\", \"time_mean\", \"time_std\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGenderExtractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table):\n",
    "    #data is very noisy. There are people who change their gender from row to row. \n",
    "    #So, lets choose most common gender for every patient from rows where he is presented.\n",
    "    gender = table[[\"idx\", \"gender\"]].groupby(\"idx\").agg(pd.Series.mode).reset_index()\n",
    "    gender[\"gender\"] = gender.gender.apply(lambda x: 1 if(x=='М') else 0)\n",
    "    self.gender = gender[[\"idx\", \"gender\"]].copy()\n",
    "    \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    return X.merge(self.gender, how='left', on='idx')[[\"gender\"]] #.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleGenderExtractor(df_a).fit_transform(df_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nearesttime_table(df, X, time_field):\n",
    "    \n",
    "    #generate sequence numbers of CT and analysis\n",
    "    df = df.sort_values(time_field)\n",
    "    df[\"cumcount_df\"] = df.groupby('idx').cumcount()\n",
    "\n",
    "\n",
    "    X[\"cumcount_X\"] = X.groupby('idx').cumcount()\n",
    "    X[\"DATE\"] = X[\"DATE\"].apply(parseDate) \n",
    "    \n",
    "    #cartesian product inside groups\n",
    "    combined = X.merge(df, on = \"idx\", how=\"left\")\n",
    "    \n",
    "    #choose nearest analysis to CT\n",
    "    combined[\"time_dif\"] = np.abs(combined[time_field].values - combined.DATE.values)\n",
    "    nearest_table = combined[[\"idx\", \"cumcount_X\", \"cumcount_df\", \"time_dif\"]].set_index(\"cumcount_df\").groupby([\"idx\", \"cumcount_X\"]).idxmin()\n",
    "    \n",
    "    nearest_table = nearest_table.reset_index()\n",
    "    \n",
    "    nearest_table = nearest_table[~nearest_table.time_dif.isna()].copy()\n",
    "    \n",
    "    nearest_table[\"cumcount_df\"] = nearest_table[\"time_dif\"].astype(\"int\")\n",
    "    \n",
    "    return df, X, nearest_table[[\"idx\", \"cumcount_X\", \"cumcount_df\"]]\n",
    "\n",
    "\n",
    "def match_features(match_table, feautes_table, feature_names, new_names):\n",
    "    rename_dict = dict(zip(feature_names, new_names))\n",
    "    features = match_table[[\"idx\", \"cumcount_X\", \"cumcount_df\"]].merge( feautes_table[[\"idx\", \"cumcount_df\"] + feature_names],                                             \n",
    "                                                          on = [\"idx\", \"cumcount_df\"], how = \"left\")\n",
    "    \n",
    "    features = features.rename(columns = rename_dict)\n",
    "    \n",
    "    return features[[\"idx\", \"cumcount_X\"] + new_names]\n",
    "\n",
    "\n",
    "class NearAntiBodyExtractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table):\n",
    "    self.data = table[table[\"samples_type_id\"]=='Кровь, цельная (сыворотка)'][[\"idx\", \"get_date_at\", \"IgG\", \"IgM\"]]#.copy()\n",
    "\n",
    "    self.time_field = \"get_date_at\"\n",
    "    \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X = X.copy()\n",
    "    df = self.data[self.data.idx.isin(set(X.idx))].copy()\n",
    "    \n",
    "    df[self.time_field] = df[self.time_field].apply(parseDate)  \n",
    "    \n",
    "    #find the nearest analysis and one before and one after\n",
    "    df, X, match_table = generate_nearesttime_table(df, X, self.time_field)\n",
    "    \n",
    "    current = match_features(match_table, df, [\"get_date_at\", 'IgG', 'IgM'], [\"td_blood_a\", 'IgG', 'IgM'])\n",
    "    match_table[\"cumcount_df\"] -= 1\n",
    "    backward = match_features(match_table, df, [\"get_date_at\", 'IgG', 'IgM'], ['td_bloodb', 'IgGb', 'IgMb'])\n",
    "    match_table[\"cumcount_df\"] += 2\n",
    "    forward = match_features(match_table, df, [\"get_date_at\", 'IgG', 'IgM'], ['td_bloodf', 'IgGf', 'IgMf'])\n",
    "    \n",
    "    features = current.merge(backward, how=\"left\", on=[\"idx\", \"cumcount_X\"]).merge(forward, how=\"left\", on=[\"idx\", \"cumcount_X\"])\n",
    "    \n",
    "    features = features.merge(X[[\"idx\", \"cumcount_X\", \"DATE\"]], how=\"left\", on=[\"idx\", \"cumcount_X\"])\n",
    "    \n",
    "    #change absolute time on relative time differences\n",
    "    features[\"td_bloodb\"] = np.abs(features.td_bloodb - features.td_blood_a)\n",
    "    features[\"td_bloodf\"] = np.abs(features.td_bloodf - features.td_blood_a)\n",
    "    features[\"td_blood_a\"] = np.abs(features.DATE - features.td_blood_a)\n",
    "    \n",
    "    features = X.merge(features.drop([\"DATE\"], axis=1), how=\"left\", on=[\"idx\", \"cumcount_X\"])\n",
    "    \n",
    "    #out of bound Nan coding\n",
    "    return features[['td_blood_a', 'IgG', 'IgM', 'td_bloodb', 'IgGb', 'IgMb', 'td_bloodf', 'IgGf', 'IgMf']].fillna(-10**9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NearAntiBodyExtractor(df_a).fit_transform(df_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = pd.read_csv(\"data_for_science_death_2020-07-27.csv\", sep=';')\n",
    "\n",
    "#time between patient's CT and his death\n",
    "\n",
    "class TTDExtractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table):\n",
    "    self.de = table.copy()\n",
    "    \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X = X.copy()\n",
    "    \n",
    "    #Dataset have people with more than one CT in the same day \n",
    "    X[\"id\"] = np.arange(len(X)) \n",
    "    \n",
    "    md = X.merge(self.de, on=\"idx\", how=\"inner\")\n",
    "    \n",
    "    md[\"ttd\"] = md.DEATH_DATE.apply(parseDate) - md.DATE.apply(parseDate)\n",
    "    md = md.drop_duplicates()\n",
    "    \n",
    "    rez = X.merge(md[[\"id\", \"ttd\"]], on=\"id\", how=\"left\").fillna(-1)\n",
    "    \n",
    "    return rez[[\"ttd\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = pd.read_csv(\"data_for_science_LIs_2020-07-27.csv\", encoding='utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseValue(value):\n",
    "    bs = ('-', '----', '--')\n",
    "    val = value.split('_')[0].replace(',','.')\n",
    "    \n",
    "    try:\n",
    "        return float(val)\n",
    "    except ValueError:\n",
    "        return -1\n",
    "\n",
    "def process_features(df, top_k, test2id):\n",
    "    l = [-1.0]*(len(top_k)+1)\n",
    "    tim = df.TEST_TIME.values[0]\n",
    "    \n",
    "    #l[0] = idx\n",
    "    l[0] = parseDate(tim.split()[0])\n",
    "    \n",
    "    for name, value in zip(df.FIXED_NAME, df.TEST_VAL):\n",
    "        if(name in test2id):\n",
    "            l[test2id[name] + 1] = parseValue(value)\n",
    "\n",
    "    return pd.Series(data=l, index=([\"get_date_at\"] + top_k[::]))\n",
    "\n",
    "\n",
    "class NearImprovedBloodExtractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table, k):\n",
    "    \n",
    "    bloods = [\"Общий клинический анализ крови (общий анализ + СОЭ + лейкоцитарная формула + тромбоциты)\",\n",
    "          \"Общий клинический анализ крови (общий анализ + СОЭ), микроскопическое исследование мазка крови при выявлении патологии\",\n",
    "          \"Общий клинический анализ крови (скрининг)\",\n",
    "          \"Клинический анализ крови\"]\n",
    "\n",
    "    table = table[table.ISSL_NAME.isin(bloods)].copy()\n",
    "    table[\"FIXED_NAME\"] = table[\"TEST_NAME\"].apply(lambda x:x.split('(')[0].strip())\n",
    "    \n",
    "    #determine most common values in analyses\n",
    "    series = table.FIXED_NAME.value_counts()\n",
    "    \n",
    "    self.top_k = []\n",
    "    self.k = k\n",
    "    \n",
    "    for i, x in enumerate(series.index):\n",
    "        if(i<k):\n",
    "            self.top_k.append(x)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "            \n",
    "            \n",
    "    #determine values order\n",
    "    self.test2id = {k:i for i, k in enumerate(self.top_k)}\n",
    "    \n",
    "    #for each patient transform each anlysis in more convinient form and save into a table\n",
    "    \n",
    "    self.tbp = table.groupby(by = [\"idx\", \"TEST_TIME\"]).apply(lambda x: process_features(x, self.top_k, self.test2id)).reset_index()\n",
    "    \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X = X.copy()\n",
    "    \n",
    "    df = self.tbp[self.tbp.idx.isin(set(X.idx))].copy()\n",
    "\n",
    "    #find the nearest analysis \n",
    "    time_field = \"get_date_at\" \n",
    "\n",
    "    df, X, match_table = generate_nearesttime_table(df, X, time_field)\n",
    "    \n",
    "    #match KT with near analysis\n",
    "    features = match_table.merge(df, on = [\"idx\", \"cumcount_df\"], how = \"left\")\n",
    "    ans = X.merge(features, on = [\"idx\", \"cumcount_X\"], how = \"left\")\n",
    "    \n",
    "    #fix time, get right columns, fill Nan out of bound\n",
    "    ans[\"td_blood\"] = np.abs(ans[\"DATE\"].values - ans[\"get_date_at\"].values)\n",
    "    \n",
    "    feature_names = [\"td_blood\"] + self.top_k\n",
    "    \n",
    "    return ans[feature_names].fillna(-10**9)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C protein was one of the most important feature in the competition\n",
    "#I extracted information about 3 nearest results to CT.\n",
    "class NearCproteinExtractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table):\n",
    "    self.table = table[table.ISSL_NAME == \"Определение белков острой фазы С-реактивный белок\"].copy()\n",
    "\n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    #TODO в оригинале были разницы С с центральным без модулей.\n",
    "    X = X.copy()\n",
    "    df = table[table.idx.isin(set(X.idx))].copy()\n",
    "\n",
    "    time_field = \"TEST_TIME\"\n",
    "\n",
    "    df[time_field] = df[time_field].apply(lambda tim: parseDate(tim.split()[0]))\n",
    "\n",
    "    df = df[[\"idx\", \"TEST_VAL\", time_field]]\n",
    "    df[\"TEST_VAL\"] = df[\"TEST_VAL\"].apply(parseValue)\n",
    "    \n",
    "    \n",
    "    #find the nearest C protein analysis\n",
    "    df, X, match_table = generate_nearesttime_table(df, X, time_field)\n",
    "    \n",
    "    #retrieve information about the nearest, one before and one after\n",
    "    current = match_features(match_table, df, [\"TEST_TIME\", \"TEST_VAL\"], [\"td_C\", \"C\"])\n",
    "    match_table[\"cumcount_df\"] -= 1\n",
    "    backward = match_features(match_table, df, [\"TEST_TIME\", \"TEST_VAL\"], [\"td_C_backward\", \"C_backward\"])\n",
    "    match_table[\"cumcount_df\"] += 2\n",
    "    forward = match_features(match_table, df, [\"TEST_TIME\", \"TEST_VAL\"], [\"td_C_forward\", \"C_forward\"])\n",
    "    \n",
    "    #calculate time differences\n",
    "    features = current.merge(backward, how=\"left\", on=[\"idx\", \"cumcount_X\"]).merge(forward, how=\"left\", on=[\"idx\", \"cumcount_X\"])\n",
    "\n",
    "    features = features.merge(X[[\"idx\", \"cumcount_X\", \"DATE\"]], how=\"left\", on=[\"idx\", \"cumcount_X\"])\n",
    "    \n",
    "    \n",
    "    features[\"td_C_backward\"] = np.abs(features.td_C - features.td_C_backward)\n",
    "    features[\"td_C_forward\"] = np.abs(features.td_C - features.td_C_forward)\n",
    "    features[\"td_C\"] = np.abs(features.DATE - features.td_C)\n",
    "    \n",
    "    \n",
    "    features = X.merge(features.drop([\"DATE\"], axis=1), how=\"left\", on=[\"idx\", \"cumcount_X\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return features[[\"td_C\", \"C\", \"td_C_backward\", \"C_backward\", \"td_C_forward\", \"C_forward\"]].fillna(-10**9)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_diag = pd.read_csv(\"data_for_science_diagnoses_2020-07-27.csv\", sep=';')\n",
    "\n",
    "#Does patient have any chronical disease\n",
    "class VerySimpleDiasesExctractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table):\n",
    "    self.I_pat = set(table[table.DIAGNOSIS.apply(lambda x:x[0]=='I')].idx) \n",
    "    self.E_pat = set(table[table.DIAGNOSIS.apply(lambda x:x[0]=='E')].idx)\n",
    "    \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X = X.copy()\n",
    "    d = {'I':np.zeros((len(X),)), 'E':[0]*np.zeros((len(X),)) }\n",
    "    d['I'][X.idx.isin(self.I_pat)] = 1\n",
    "    d['E'][X.idx.isin(self.E_pat)] = 1\n",
    "    \n",
    "    return pd.DataFrame.from_dict(d) #.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain information about patien resperatory diseases in \"bag of words\" form\n",
    "#use time between diagnosis and CT instead of ones \n",
    "def extractTime2diag(d, top_k, test2id, x):\n",
    "    if(x[\"idx\"] in d):\n",
    "        dd = d[x[\"idx\"]]\n",
    "        vals = np.zeros((len(top_k), )) - 10**9 \n",
    "        for k, v in dd.items():\n",
    "            dif = np.array(v)\n",
    "            diffs = x[\"time\"] - dif\n",
    "            vals[test2id[k]] = diffs[np.abs(diffs).argsort()[0]] #.min()\n",
    "        \n",
    "        return pd.Series(data=vals, index=top_k[::])\n",
    "    else:\n",
    "        return pd.Series(data=[-10**9]*len(top_k), index=top_k[::])\n",
    "\n",
    "#I assumed that chronic resperatory diseases have the greatest impact on covid severity\n",
    "#so I extracted more detailed information about them\n",
    "class ResperatoryDiasesExctractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table, k):\n",
    "    table = table[table.DIAGNOSIS.apply(lambda x:x[0]=='J')].copy()\n",
    "    series = table.DIAGNOSIS.value_counts()\n",
    "    \n",
    "    self.top_k = []\n",
    "    \n",
    "    #choose top k most common resperatory diseases\n",
    "\n",
    "    for i, x in enumerate(series.index):\n",
    "        if(i<k):\n",
    "            self.top_k.append(x)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    self.test2id = {k:i for i, k in enumerate(self.top_k)}\n",
    "    \n",
    "    #Transform table to dict\n",
    "    \n",
    "    self.d = {}\n",
    "    for idx, g in table.groupby(\"idx\"):\n",
    "        self.d[idx] = {}\n",
    "        for tim, diag in zip(g.DATE, g.DIAGNOSIS):\n",
    "            pt = parseDate(tim.split()[0])\n",
    "            if(diag not in self.top_k):\n",
    "                continue\n",
    "            if diag in self.d[idx]:\n",
    "                self.d[idx][diag].append(pt)\n",
    "            else:\n",
    "                self.d[idx][diag] = [pt]\n",
    "                \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X = X.copy()\n",
    "    X[\"time\"] = X.DATE.apply(parseDate)\n",
    "    \n",
    "    return X.apply(lambda x: extractTime2diag(self.d, self.top_k, self.test2id, x), axis=1) #.values.tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimum aggregation of previous features group by time. \n",
    "#Return time when a disease was diagnosed and categorical encoded disease\n",
    "\n",
    "def extractNearestDisease(d, top_k, test2id, x):\n",
    "    if(x[\"idx\"] in d):\n",
    "        dd = d[x[\"idx\"]]\n",
    "        vals = np.zeros((len(top_k), )) - 10**9 \n",
    "        for k, v in dd.items():\n",
    "            dif = np.array(v)\n",
    "            diffs = x[\"time\"] - dif\n",
    "            vals[test2id[k]] = diffs[np.abs(diffs).argsort()[0]] #.min()\n",
    "            \n",
    "        min_i = np.abs(vals).argsort()[0]\n",
    "        md = vals[min_i]\n",
    "        \n",
    "        return pd.Series(data=[md, min_i], index=[\"time2near_diag\", \"near_diag\"])\n",
    "    else:\n",
    "        return pd.Series(data=[-10**9]*2, index=[\"time2near_diag\", \"near_diag\"])\n",
    "\n",
    "class ResperatoryDiasesNearExctractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table, k):\n",
    "    table = table[table.DIAGNOSIS.apply(lambda x:x[0]=='J')].copy()\n",
    "    series = table.DIAGNOSIS.value_counts()\n",
    "    self.top_k = []\n",
    "\n",
    "    for i, x in enumerate(series.index):\n",
    "        if(i<k):\n",
    "            self.top_k.append(x)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    self.test2id = {k:i for i, k in enumerate(self.top_k)}\n",
    "\n",
    "    self.d = {}\n",
    "    for idx, g in table.groupby(\"idx\"):\n",
    "        self.d[idx] = {}\n",
    "        for tim, diag in zip(g.DATE, g.DIAGNOSIS):\n",
    "            pt = parseDate(tim.split()[0])\n",
    "            if(diag not in self.top_k):\n",
    "                continue\n",
    "            if diag in self.d[idx]:\n",
    "                self.d[idx][diag].append(pt)\n",
    "            else:\n",
    "                self.d[idx][diag] = [pt]\n",
    "                \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X = X.copy()\n",
    "    X[\"time\"] = X.DATE.apply(parseDate)\n",
    "    X = X.apply(lambda x: extractNearestDisease(self.d, self.top_k, self.test2id, x), axis=1) \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other disease I percieved as not very important. So, for them I encoded only the nearest diagnosed disease \n",
    "\n",
    "class OtherDiasesNearExctractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table, k):\n",
    "    table = table[table.DIAGNOSIS.apply(lambda x:(x[0]=='E')|(x[0]=='I'))].copy()\n",
    "    series = table.DIAGNOSIS.value_counts()\n",
    "    \n",
    "    self.top_k = []\n",
    "\n",
    "    for i, x in enumerate(series.index):\n",
    "        if(i<k):\n",
    "            self.top_k.append(x)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    self.test2id = {k:i for i, k in enumerate(self.top_k)}\n",
    "\n",
    "    self.d = {}\n",
    "    for idx, g in table.groupby(\"idx\"):\n",
    "        self.d[idx] = {}\n",
    "        for tim, diag in zip(g.DATE, g.DIAGNOSIS):\n",
    "            pt = parseDate(tim.split()[0])\n",
    "            if(diag not in self.top_k):\n",
    "                continue\n",
    "            if diag in self.d[idx]:\n",
    "                self.d[idx][diag].append(pt)\n",
    "            else:\n",
    "                self.d[idx][diag] = [pt]\n",
    "                \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X = X.copy()\n",
    "    X[\"time\"] = X.DATE.apply(parseDate)\n",
    "    X = X.apply(lambda x: extractNearestDisease(self.d, self.top_k, self.test2id, x), axis=1) \n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drug = pd.read_csv('data_for_science_drugs_2020-07-27.csv', encoding='utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "#some patients have prescriptions. I encoded that with bag of words. I left only the nearest prescription for each CT.\n",
    "\n",
    "def bagOfWordsRecipe(d, drugs, x):\n",
    "    if(x[\"idx\"] in d):\n",
    "        \n",
    "        dif = np.array([y[0] for y in d[x[\"idx\"]]])\n",
    "        \n",
    "        diffs = x[\"time\"] - dif\n",
    "        min_i = np.argsort(np.abs(diffs))[0]\n",
    "        \n",
    "        df = diffs[min_i]\n",
    "        rv = [df] + (d[x[\"idx\"]][min_i][1:]) + [len(dif)]\n",
    "        \n",
    "        return pd.Series(data=rv, index=([\"td_Rec\"] + drugs + [\"n_recep\"]))\n",
    "    else:\n",
    "        return pd.Series(data=[-10**9] + [0]*(len(drugs)+1), index=([\"td_Rec\"] + drugs + [\"n_recep\"]))\n",
    "\n",
    "#clear string\n",
    "def delSym(st):\n",
    "    s = set({'!', '#',\n",
    " '%',\n",
    " '&',\n",
    " \"'\",\n",
    " '(',\n",
    " ')',\n",
    " '*',\n",
    " '+',\n",
    " ',',\n",
    " '-',\n",
    " '.',\n",
    " '/', \n",
    " ':',\n",
    " ';',\n",
    " '<',\n",
    " '=',\n",
    " '>',\n",
    " '?', \n",
    " '[',\n",
    " '\\\\',\n",
    " ']',\n",
    " '^',\n",
    " '_',})\n",
    "    return ''.join([x if x not in s else ' ' for x in st])\n",
    "\n",
    "class ReceipeExtractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, df_drugs, t_hold = 5):\n",
    "    \n",
    "    #I left only tokens with count higher than t_hold value\n",
    "    rep = df_drug.RECIPE.apply(lambda x: delSym(x).split()).tolist()\n",
    "    mc = Counter(list(filter(lambda x:x[0].isupper(), itertools.chain.from_iterable(rep)))).most_common()\n",
    "    self.drugs = [x for x, c in mc if c>t_hold]\n",
    "    s_drugs = set(self.drugs)\n",
    "    test2id = {k:i for i, k in enumerate(self.drugs)}\n",
    "    \n",
    "    #gather information from table to more convinient form in dict\n",
    "    self.d = {}\n",
    "    for idx, g in df_drug.groupby(\"idx\"):\n",
    "        self.d[idx] = []\n",
    "        for tim, gg in g.groupby(\"DATE\"):\n",
    "            val = ' '.join(gg.RECIPE.tolist())\n",
    "            l = [0]*(len(self.drugs)+1)\n",
    "            l[0] = parseDate(tim.split()[0])\n",
    "            for t in delSym(val).split():\n",
    "                if t in s_drugs:\n",
    "                    l[test2id[t]] = 1\n",
    "            self.d[idx].append(l)\n",
    "    \n",
    "    \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X = X.copy()\n",
    "    X[\"time\"] = X.DATE.apply(parseDate)\n",
    "    \n",
    "    return X.apply(lambda x: bagOfWordsRecipe(self.d, self.drugs, x), axis=1) #.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Was a patient hospitalized \n",
    "class HospitalFlagExtractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table):\n",
    "    self.hosp_id = set(df_a[df_a.mu_type.apply(lambda x: 'больница' in x)].idx)\n",
    "    \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X = X.copy()\n",
    "    X[\"hosp\"] = X.idx.apply(lambda x: 1 if x in self.hosp_id else 0)\n",
    "    return X[[\"hosp\"]] #.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Swabs data is very nosy and results do not correlate with severity\n",
    "#Time data have the least noise. And therefore I  meticulously generated features based on time\n",
    "\n",
    "#Time between first and last swab, mean time and std between swab.\n",
    "\n",
    "#Also instituion where swabs was gotten is quiet reliable. I also retirieved that.\n",
    "\n",
    "#Swabs results is rather noisy but I also added them just in case\n",
    "\n",
    "def mazokStatFunction(d, header, infty, x):\n",
    "    if(x[\"idx\"] in d):\n",
    "        places = d[x[\"idx\"]][1]\n",
    "        pf = []\n",
    "        pfn = []\n",
    "        for p in places:\n",
    "            if(len(p)==0):\n",
    "                pf.append(infty)\n",
    "                pfn.append(0)\n",
    "            else:\n",
    "                dif = np.array(p)\n",
    "                diffs = np.abs(x[\"time\"] - dif)\n",
    "                near_id =  diffs.argsort()[0]\n",
    "                pf.append(x[\"time\"] - dif[near_id])\n",
    "                pfn.append(len(diffs))\n",
    "        \n",
    "        trues = d[x[\"idx\"]][2]\n",
    "        if (len(trues) == 0):\n",
    "            tf = infty\n",
    "        else:\n",
    "            dif = np.array(trues)\n",
    "            diffs = np.abs(x[\"time\"] - dif)\n",
    "            near_id =  diffs.argsort()[0]\n",
    "            tf = x[\"time\"] - dif[near_id]\n",
    "        \n",
    "        features = d[x[\"idx\"]][0][::]\n",
    "        \n",
    "        features[1] = features[1] - x[\"time\"]\n",
    "        features[2] = features[2] - x[\"time\"]\n",
    "        \n",
    "        features += (pf + pfn)\n",
    "        features += [tf]\n",
    "        \n",
    "        \n",
    "        return pd.Series(data= features , index=header)\n",
    "    else:\n",
    "        return pd.Series(data=[-1]*len(header), index=header)\n",
    "\n",
    "class MazokExctractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table):\n",
    "    \n",
    "    table = table[table.samples_type_id == 'Мазок/отделяемое из носоглотки и ротоглотки']\n",
    "    self.top_k = []\n",
    "    k = 20\n",
    "    series = table.mu_type.value_counts()\n",
    "\n",
    "    for i, x in enumerate(series.index):\n",
    "        if(i<k):\n",
    "            self.top_k.append(x)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    self.ot2id = {k:i for i, k in enumerate(self.top_k)}\n",
    "    self.top_k_n = [x+'_n' for x in self.top_k]\n",
    "\n",
    "            \n",
    "    self.d = {}\n",
    "    for idx, g in table.groupby(\"idx\"):\n",
    "        times = g.get_date_at.apply(parseDate)\n",
    "        if(len(g)==0):\n",
    "            continue\n",
    "        n = len(times)\n",
    "        mi = times.min()\n",
    "        ma = times.max()\n",
    "\n",
    "        if(n != 1):\n",
    "            dif = np.diff(times)\n",
    "            mean = dif.mean()\n",
    "            std = dif.std()\n",
    "        else:\n",
    "            mean = -1\n",
    "            std = -1\n",
    "\n",
    "        ob_flag = []\n",
    "\n",
    "        for x, tim in zip(g.samples_result_id, g.get_date_at):\n",
    "            if x == 'ОБНАРУЖЕНО':\n",
    "                ob_flag.append(parseDate(tim))\n",
    "\n",
    "        place = [[] for _ in range(len(self.top_k))]\n",
    "\n",
    "        for x, tim in zip(g.mu_type, g.get_date_at):\n",
    "            place[self.ot2id[x]].append(parseDate(tim))\n",
    "\n",
    "        features = [n, mi, ma, ma-mi, mean, std]\n",
    "\n",
    "        self.d[idx] = (features, place, ob_flag)\n",
    "        \n",
    "    self.infty = -10**9\n",
    "    self.header = [\"n\", \"tt_min\", \"tt_max\", \"max-min\", \"mean\", \"std\"] + self.top_k + self.top_k_n + [\"tt_true\"]\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X = X.copy()\n",
    "    X[\"time\"] = X.DATE.apply(parseDate)\n",
    "    \n",
    "    return X.apply(lambda x: mazokStatFunction(self.d, self.header, self.infty, x), axis=1) #.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from time of antibody blood tests. Just in case. \n",
    "def antibodyStatFunction(d, header, infty, x):\n",
    "    if(x[\"idx\"] in d):\n",
    "        places = d[x[\"idx\"]][1]\n",
    "        pf = []\n",
    "        pfn = []\n",
    "        for p in places:\n",
    "            if(len(p)==0):\n",
    "                pf.append(infty)\n",
    "                pfn.append(0)\n",
    "            else:\n",
    "                dif = np.array(p)\n",
    "                diffs = np.abs(x[\"time\"] - dif)\n",
    "                near_id =  diffs.argsort()[0]\n",
    "                pf.append(x[\"time\"] - dif[near_id])\n",
    "                pfn.append(len(diffs))\n",
    "        \n",
    "        \n",
    "        features = d[x[\"idx\"]][0][::]\n",
    "        \n",
    "        features[1] = features[1] - x[\"time\"]\n",
    "        features[2] = features[2] - x[\"time\"]\n",
    "        \n",
    "        features += (pf + pfn)\n",
    "        \n",
    "        return pd.Series(data= features , index=header)\n",
    "    else:\n",
    "        return pd.Series(data=[-1]*len(header), index=header)\n",
    "\n",
    "class AntibodyStatExctractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table):\n",
    "    \n",
    "    table = table[table.samples_type_id == 'Кровь, цельная (сыворотка)']\n",
    "    self.top_k = []\n",
    "    k = 20\n",
    "    series = table.mu_type.value_counts()\n",
    "\n",
    "    for i, x in enumerate(series.index):\n",
    "        if(i<k):\n",
    "            self.top_k.append(x)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    self.ot2id = {k:i for i, k in enumerate(self.top_k)}\n",
    "    self.top_k_n = [x+'_n' for x in self.top_k]\n",
    "\n",
    "            \n",
    "    self.d = {}\n",
    "    for idx, g in table.groupby(\"idx\"):\n",
    "        times = g.get_date_at.apply(parseDate)\n",
    "        if(len(g)==0):\n",
    "            continue\n",
    "        n = len(times)\n",
    "        mi = times.min()\n",
    "        ma = times.max()\n",
    "\n",
    "        if(n != 1):\n",
    "            dif = np.diff(times)\n",
    "            mean = dif.mean()\n",
    "            std = dif.std()\n",
    "        else:\n",
    "            mean = -1\n",
    "            std = -1\n",
    "\n",
    "        place = [[] for _ in range(len(self.top_k))]\n",
    "\n",
    "        for x, tim in zip(g.mu_type, g.get_date_at):\n",
    "            place[self.ot2id[x]].append(parseDate(tim))\n",
    "\n",
    "        features = [n, mi, ma, ma-mi, mean, std]\n",
    "\n",
    "        self.d[idx] = (features, place)\n",
    "        \n",
    "    self.infty = -10**9\n",
    "    self.header = [\"n\", \"tt_min\", \"tt_max\", \"max-min\", \"mean\", \"std\"] + self.top_k + self.top_k_n\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X = X.copy()\n",
    "    X[\"time\"] = X.DATE.apply(parseDate)\n",
    "    \n",
    "    return X.apply(lambda x: antibodyStatFunction(self.d, self.header, self.infty, x), axis=1) #.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we extracts here which diseases therepists assumed when they got swabs and blood test.\n",
    "#how long that diseases was diagnosed, how many times eash disease was assumed \n",
    "#that information quiet noisy and not very reliable but may have some impact on predictions\n",
    "\n",
    "def diseaseStatExtractor(d, code2id, header, x):\n",
    "    if(x[\"idx\"] in d):\n",
    "        \n",
    "        dd = d[x[\"idx\"]]\n",
    "        features = [-10**9]*(len(header)//2)\n",
    "        features_n = [0]*(len(header)//2)\n",
    "        \n",
    "        for k, v in dd.items():            \n",
    "            dif = np.array(v)\n",
    "            \n",
    "            diffs = x[\"time\"] - dif\n",
    "            near_id =  np.abs(diffs).argsort()[0]\n",
    "            \n",
    "            if not (k in code2id):\n",
    "                features[-1]  =  diffs[near_id]\n",
    "                features_n[-1] += len(dif)\n",
    "            else:\n",
    "                features[code2id[k]] = diffs[near_id]\n",
    "                features_n[ code2id[k] ] = len(dif)\n",
    "        \n",
    "        sn = sum(features)\n",
    "        features_n = [x/sn for x in features_n]\n",
    "        \n",
    "        return pd.Series(data= features + features_n, index=header)\n",
    "    else:\n",
    "        return pd.Series(data=[-10**9]*len(header), index=header)\n",
    "\n",
    "    \n",
    "class ReasonExctractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table, th = 4):\n",
    "    self.d = {}\n",
    "    self.th = th\n",
    "    for idx, g in table.groupby(\"idx\"):\n",
    "        self.d[idx] = {}\n",
    "        for tim, code in zip(g.get_date_at, g.mkb10_code):\n",
    "            if code in self.d[idx]:\n",
    "                self.d[idx][code].append(parseDate(tim))\n",
    "            else:\n",
    "                self.d[idx][code] = [parseDate(tim)]\n",
    "    \n",
    "        \n",
    "  def fit(self, X, y = None):\n",
    "    df_r = X.copy()\n",
    "    s = set(df_r.idx)\n",
    "    tb = df_a[df_a.idx.isin(s)]\n",
    "    tb = tb[[\"idx\", \"mkb10_code\"]].drop_duplicates()\n",
    "    good_code = []\n",
    "    #filter most common diseases\n",
    "    series = tb.mkb10_code.value_counts()\n",
    "    for i in series.index:\n",
    "        if(series[i]>self.th):\n",
    "            good_code.append(i)\n",
    "    \n",
    "    self.code2id = {code:i for i, code in enumerate(good_code)}\n",
    "    #header of resulting dataset. times + counts + stats about rare diseases\n",
    "    self.header = good_code + [x+'_n' for x in good_code] + [\"rare\"] + [\"rare_n\"]\n",
    "    \n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X = X.copy()\n",
    "    X[\"time\"] = X.DATE.apply(parseDate)\n",
    "    \n",
    "    return X.apply(lambda x: diseaseStatExtractor(self.d, self.code2id, self.header, x), axis=1) #.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the nearest assumed diagnosis to CT\n",
    "def nearAssumeExtracor(d, code2id, header, x):\n",
    "    if(x[\"idx\"] in d):\n",
    "        \n",
    "        dd = d[x[\"idx\"]]\n",
    "        features = [-10**9]*len(header)\n",
    "        \n",
    "        for k, v in dd.items():\n",
    "            k = k[0]\n",
    "            dif = np.array(v)\n",
    "            \n",
    "            diffs = x[\"time\"] - dif\n",
    "            near_id =  np.abs(diffs).argsort()[0]\n",
    "            \n",
    "            if not (k in code2id):\n",
    "                features[-1]  =  diffs[near_id]\n",
    "            else:\n",
    "                features[code2id[k]] = diffs[near_id]\n",
    "        \n",
    "            \n",
    "        \n",
    "        return pd.Series(data= features , index=header)\n",
    "    else:\n",
    "        return pd.Series(data=[-10**9]*len(header), index=header)\n",
    "\n",
    "    \n",
    "class ReasonExctractorLatter(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table, th = 4):\n",
    "    self.d = {}\n",
    "    self.th = th\n",
    "    for idx, g in table[[\"idx\",\"get_date_at\", \"mkb10_code\"]].fillna('#').groupby(\"idx\"):\n",
    "        self.d[idx] = {}\n",
    "        for tim, code in zip(g.get_date_at, g.mkb10_code):\n",
    "            #print(code)\n",
    "            code = code[0]\n",
    "            if code in self.d[idx]:\n",
    "                self.d[idx][code].append(parseDate(tim))\n",
    "            else:\n",
    "                self.d[idx][code] = [parseDate(tim)]\n",
    "    \n",
    "        \n",
    "  def fit(self, X, y = None):\n",
    "    df_r = X.copy()\n",
    "    s = set(df_r.idx)\n",
    "    tb = df_a[df_a.idx.isin(s)]\n",
    "    tb = tb[[\"idx\", \"mkb10_code\"]].drop_duplicates().copy()\n",
    "    tb[\"code\"] = tb.fillna('#').mkb10_code.apply(lambda x:x[0])\n",
    "    good_code = []\n",
    "    series = tb.code.value_counts()\n",
    "    for i in series.index:\n",
    "        if(series[i]>self.th):\n",
    "            good_code.append(i)\n",
    "    \n",
    "    self.code2id = {code:i for i, code in enumerate(good_code)}\n",
    "    self.header = good_code + [\"rare\"]\n",
    "    \n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X = X.copy()\n",
    "    X[\"time\"] = X.DATE.apply(parseDate)\n",
    "    \n",
    "    return X.apply(lambda x: nearAssumeExtracor(self.d, self.code2id, self.header, x), axis=1) #.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReasonExctractorLatter(df_a).fit_transform(df_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = pd.read_csv(\"data_for_science_questions_2020-07-27.csv\", sep=';')\n",
    "#Did a patient answer the questionare about Covid 19.\n",
    "class QuestionExctractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table):\n",
    "    series = table.idx.value_counts()\n",
    "    self.d = dict(zip(series.index, series.values))\n",
    "        \n",
    "  def fit(self, X, y = None):\n",
    "    \n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X  = X.copy()\n",
    "    X[\"questions\"] = X.idx.apply(lambda x: self.d[x] if x in self.d else -1)\n",
    "    \n",
    "    return X[[\"questions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is a special analysis which is taken from a patient only if he is hospitalyzed\n",
    "#Not very much people in the datset have them. But it may have some impact on predictions\n",
    "\n",
    "def baseFeatExtract(d, top_k, x):\n",
    "    \n",
    "    if(x[\"idx\"] in d):\n",
    "        \n",
    "        dif = np.array([y[0] for y in d[x[\"idx\"]]])\n",
    "        \n",
    "        diffs = x[\"time\"] - dif\n",
    "        min_i = np.argsort(np.abs(diffs))[0]\n",
    "        \n",
    "        df = diffs[min_i]\n",
    "        rv = [df] + (d[x[\"idx\"]][min_i][1:])\n",
    "        \n",
    "        return pd.Series(data=rv, index=([\"td_blood\"] + top_k[::]))\n",
    "    else:\n",
    "        return pd.Series(data=[-10**9]*(len(top_k)+1), index=([\"td_blood\"] + top_k[::]))\n",
    "\n",
    "\n",
    "class NearDdimerExtractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table):\n",
    "    k=1\n",
    "\n",
    "    table = table[table.ISSL_NAME == \"D-димер\"]\n",
    "\n",
    "    series = table.TEST_NAME.value_counts()\n",
    "    \n",
    "    self.top_k = []\n",
    "    self.k = k\n",
    "\n",
    "    for i, x in enumerate(series.index):\n",
    "        if(i<k):\n",
    "            self.top_k.append(x)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    self.test2id = {k:i for i, k in enumerate(self.top_k)}\n",
    "    \n",
    "    self.d = {}\n",
    "    for idx, g in table.groupby(\"idx\"):\n",
    "        self.d[idx] = []\n",
    "        for tim, gg in g.groupby(\"TEST_TIME\"):\n",
    "            #print(gg)\n",
    "            l = [-1.0]*(len(self.top_k)+1)\n",
    "            l[0] = parseDate(tim.split()[0])\n",
    "            for name, value in zip(gg.TEST_NAME, gg.TEST_VAL):\n",
    "                if(name in self.test2id):\n",
    "                    l[self.test2id[name]+1] = parseValue(value)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            self.d[idx].append(l)\n",
    "    \n",
    "    \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X = X.copy()\n",
    "    X[\"time\"] = X.DATE.apply(parseDate)\n",
    "    \n",
    "    return X.apply(lambda x: baseFeatExtract(self.d, self.top_k, x), axis=1) #.values.tolist()\n",
    "\n",
    "class NearFerrumExtractor(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self, table):\n",
    "    k=1\n",
    "\n",
    "    table = table[table.ISSL_NAME == \"Исследование ферритина\"]\n",
    "\n",
    "    series = table.TEST_NAME.value_counts()\n",
    "    \n",
    "    self.top_k = []\n",
    "    self.k = k\n",
    "\n",
    "    for i, x in enumerate(series.index):\n",
    "        if(i<k):\n",
    "            self.top_k.append(x)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    self.test2id = {k:i for i, k in enumerate(self.top_k)}\n",
    "    \n",
    "    self.d = {}\n",
    "    for idx, g in table.groupby(\"idx\"):\n",
    "        self.d[idx] = []\n",
    "        for tim, gg in g.groupby(\"TEST_TIME\"):\n",
    "            #print(gg)\n",
    "            l = [-1.0]*(len(self.top_k)+1)\n",
    "            l[0] = parseDate(tim.split()[0])\n",
    "            for name, value in zip(gg.TEST_NAME, gg.TEST_VAL):\n",
    "                if(name in self.test2id):\n",
    "                    l[self.test2id[name]+1] = parseValue(value)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            self.d[idx].append(l)\n",
    "    \n",
    "    \n",
    "  def fit(self, X, y = None):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X, y = None):\n",
    "    X = X.copy()\n",
    "    X[\"time\"] = X.DATE.apply(parseDate)\n",
    "    \n",
    "    return X.apply(lambda x: baseFeatExtract(self.d, self.top_k, x), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "import random\n",
    "\n",
    "#Competition had sofisticated metric function for ordinals.\n",
    "#This score function is sort of hybrid between classification and regression.\n",
    "#Below you can see its implementation\n",
    "\n",
    "def score_func(y_true, y_pred):\n",
    "    nu = [4, 3, 2, 3, 4]\n",
    "    rez = 0\n",
    "    \n",
    "    for i in range(5):\n",
    "        y = y_pred[y_true == i]\n",
    "        rez += np.abs(i-y).mean()/nu[i]\n",
    "        \n",
    "    return 1 - rez/5\n",
    "\n",
    "scorer = make_scorer(score_func, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33333333333333337"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_custom_loss_func([0, 1, 2, 3, 4], [0, 1, 2, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can derive next things from EDA:\n",
    "#1) train and test do not intersect by patients\n",
    "#2) Train and test was collected in the same time interval.\n",
    "#3) Classes distribution do not change from train to test\n",
    "#4) Train is 3 time greater than test\n",
    "\n",
    "#let's built our validtion keeping that in mind. I used simple 4-fold cross validation by patients \n",
    "def custom_cv(train):\n",
    "    l = list(set(train.idx))\n",
    "    random.shuffle(l)\n",
    "    for sp  in np.array_split(np.array(l), 4):\n",
    "        s = set(sp)\n",
    "        yield (train[~train.idx.isin(s)].index.values, train[train.idx.isin(s)].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "#gather continious features\n",
    "\n",
    "#+0.664\n",
    "transformer_list = [(\"year\", SimpleYearExtractor( df_a)), #1\n",
    "                      (\"counts\", SimpleCountExtractor() ), #2\n",
    "                      (\"times\", SimpleTimeExtractor() ), #3\n",
    "                      (\"gender\", SimpleGenderExtractor(df_a)), #1 0.669\n",
    "                      (\"antibody\", NearAntiBodyExtractor(df_a)), #3 0.70\n",
    "                      (\"TTD\", TTDExtractor(md)), #1 0.703\n",
    "                      (\"C-protein\", NearCproteinExtractor(df_d)), #2 0.720\n",
    "                      (\"Simple Blood\", NearImprovedBloodExtractor(df_d, 40)), #41 0.725\n",
    "                      (\"History Diases\", VerySimpleDiasesExctractor(df_diag)), #2 0.726\n",
    "                      (\"Respiratory Diases\", ResperatoryDiasesExctractor(df_diag, 100)), #100 74.2\n",
    "                      (\"Receipe\", ReceipeExtractor(df_drug, 5)), #75.2 важен факт выписывания рецепта и гидрохлорин\n",
    "                      (\"Mazok\", MazokExctractor(df_a) ),  # 77.3\\\n",
    "                      (\"Reason\", ReasonExctractor(df_a)),\n",
    "                      (\"Question\", QuestionExctractor(q)),\n",
    "                      (\"D-dimer\", NearDdimerExtractor(df_d) ), \n",
    "                      (\"Ferrum\", NearFerrumExtractor(df_d)), \n",
    "                      (\"Antibody Stat\", AntibodyStatExctractor(df_a))\n",
    "                      ]\n",
    "                    \n",
    "union = FeatureUnion(transformer_list, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FeatureUnion] ......... (step 1 of 17) Processing year, total=   0.1s\n",
      "[FeatureUnion] ....... (step 2 of 17) Processing counts, total= 1.8min\n",
      "[FeatureUnion] ........ (step 3 of 17) Processing times, total= 2.0min\n",
      "[FeatureUnion] ....... (step 4 of 17) Processing gender, total=   0.1s\n",
      "[FeatureUnion] ..... (step 5 of 17) Processing antibody, total=  29.8s\n",
      "[FeatureUnion] .......... (step 6 of 17) Processing TTD, total=   0.1s\n",
      "[FeatureUnion] .... (step 7 of 17) Processing C-protein, total=  33.4s\n",
      "[FeatureUnion] . (step 8 of 17) Processing Simple Blood, total=  30.5s\n",
      "[FeatureUnion]  (step 9 of 17) Processing History Diases, total=   0.0s\n",
      "[FeatureUnion]  (step 10 of 17) Processing Respiratory Diases, total=  32.9s\n",
      "[FeatureUnion] ..... (step 11 of 17) Processing Receipe, total=  45.3s\n",
      "[FeatureUnion] ....... (step 12 of 17) Processing Mazok, total=  33.1s\n",
      "[FeatureUnion] ...... (step 13 of 17) Processing Reason, total=  57.9s\n",
      "[FeatureUnion] .... (step 14 of 17) Processing Question, total=   0.1s\n",
      "[FeatureUnion] ..... (step 15 of 17) Processing D-dimer, total=  26.2s\n",
      "[FeatureUnion] ...... (step 16 of 17) Processing Ferrum, total=  30.1s\n",
      "[FeatureUnion]  (step 17 of 17) Processing Antibody Stat, total=  30.7s\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "X_train = union.fit_transform(df_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FeatureUnion]  (step 1 of 2) Processing Other Diases Near, total=  51.0s\n",
      "[FeatureUnion]  (step 2 of 2) Processing Resperatory Diases Near, total=  58.5s\n"
     ]
    }
   ],
   "source": [
    "#gather categorical features\n",
    "\n",
    "transformer_list2 = [(\"Other Diases Near\", OtherDiasesNearExctractor(df_diag, 150)),\n",
    "                     (\"Resperatory Diases Near\", ResperatoryDiasesNearExctractor(df_diag, 100))\n",
    "                      ]\n",
    "                    \n",
    "union2 = FeatureUnion(transformer_list2, verbose = 1)\n",
    "X_train2 = union2.fit_transform(df_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[952, 954]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save catigorical features columns numbers for lgbm\n",
    "categorical_feature = [X_train.shape[1]+1, X_train.shape[1]+3]\n",
    "categorical_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stack continious and categorical features\n",
    "X_TRAIN = np.hstack([X_train, X_train2])#[:,:100] #np.hstack([X_train[:,:-12], X_train[:,[-1]]])\n",
    "y = df_r.y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 0.6646464646464647\n"
     ]
    }
   ],
   "source": [
    "#threshold baseline\n",
    "r = []\n",
    "for th in np.linspace(0, 5, 100):\n",
    "    rz = my_custom_loss_func(df_r.y.values, np.zeros(df_r.y.values.shape)+th)\n",
    "    r.append(rz)\n",
    "x = np.argsort(r)[::-1][0]\n",
    "print(x, r[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0202020202020203"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0, 5, 100)[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see best constant prediction is 2 and it lead us to the metric value 0.664\n",
    "#Predict second lung damage degree for each patient is the best option for us.\n",
    "#looks quiet unrealistic prediction in real life but for our metric it make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    55685\n",
       "0    48092\n",
       "2    21231\n",
       "3     6678\n",
       "4      603\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to direct metric optimization we can use L1 loss with proper calculated weights\n",
    "#here we calculate these weights\n",
    "\n",
    "nu = [4, 3, 2, 3, 4]\n",
    "mu = [90, 80, 35, 11, 1]\n",
    "sample_weights = np.zeros(df_r.y.values.shape)\n",
    "for i in range(5):\n",
    "    sample_weights[df_r.y.values == i] = 1/nu[i]/mu[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's find best hyperparametrs on CV for lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    param_grid = {\n",
    "        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [500]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 300, step=20),\n",
    "        #\"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100, step=5),\n",
    "        #\"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "        #\"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0, 0.1),\n",
    "        \"subsample\": trial.suggest_float(\n",
    "            \"subsample\", 0.2, 0.95, step=0.1\n",
    "        ),\n",
    "        \"subsample_freq\": trial.suggest_categorical(\"subsample_freq\", [1]),\n",
    "        \"colsample_bytree\": trial.suggest_float(\n",
    "            \"colsample_bytree\", 0.2, 0.95, step=0.1\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    lgbm = LGBMRegressor(**param_grid)\n",
    "    cvr = []\n",
    "    \n",
    "    for ti, di in custom_cv(df_r):\n",
    "        Xt = X_TRAIN[ti]\n",
    "        yt = y[ti]\n",
    "\n",
    "\n",
    "        Xd = X_TRAIN[di]\n",
    "        yd = y[di]\n",
    "        \n",
    "        lgbm.fit(\n",
    "            Xt,\n",
    "            yt, sample_weight = sample_weights[ti], categorical_feature=categorical_feature\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "        ypred = lgbm.predict(Xd)\n",
    "\n",
    "        cvr.append(my_custom_loss_func(yd, ypred))\n",
    "\n",
    "\n",
    "    return np.mean(cvr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-10 13:34:06,818]\u001b[0m A new study created in memory with name: LGBM Classifier\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:35:15,198]\u001b[0m Trial 0 finished with value: 0.7772348573486314 and parameters: {'n_estimators': 500, 'learning_rate': 0.1211019352227604, 'num_leaves': 20, 'min_child_samples': 75, 'min_split_gain': 0.021916234774556634, 'subsample': 0.5, 'subsample_freq': 1, 'colsample_bytree': 0.7}. Best is trial 0 with value: 0.7772348573486314.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:36:27,473]\u001b[0m Trial 1 finished with value: 0.7814932479640063 and parameters: {'n_estimators': 500, 'learning_rate': 0.08464191103644679, 'num_leaves': 120, 'min_child_samples': 90, 'min_split_gain': 0.04931078954000399, 'subsample': 0.6000000000000001, 'subsample_freq': 1, 'colsample_bytree': 0.4}. Best is trial 1 with value: 0.7814932479640063.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:37:08,797]\u001b[0m Trial 2 finished with value: 0.7797151426191735 and parameters: {'n_estimators': 500, 'learning_rate': 0.020000602458769952, 'num_leaves': 260, 'min_child_samples': 55, 'min_split_gain': 0.0987103537839425, 'subsample': 0.2, 'subsample_freq': 1, 'colsample_bytree': 0.30000000000000004}. Best is trial 1 with value: 0.7814932479640063.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:38:16,072]\u001b[0m Trial 3 finished with value: 0.7808446916128557 and parameters: {'n_estimators': 500, 'learning_rate': 0.031191046181847634, 'num_leaves': 160, 'min_child_samples': 55, 'min_split_gain': 0.0063459897487817, 'subsample': 0.7, 'subsample_freq': 1, 'colsample_bytree': 0.2}. Best is trial 1 with value: 0.7814932479640063.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:40:02,112]\u001b[0m Trial 4 finished with value: 0.7647279987137465 and parameters: {'n_estimators': 500, 'learning_rate': 0.2584178928980305, 'num_leaves': 280, 'min_child_samples': 45, 'min_split_gain': 0.03895933219638364, 'subsample': 0.5, 'subsample_freq': 1, 'colsample_bytree': 0.7}. Best is trial 1 with value: 0.7814932479640063.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:40:57,715]\u001b[0m Trial 5 finished with value: 0.776053578977943 and parameters: {'n_estimators': 500, 'learning_rate': 0.10195729768493705, 'num_leaves': 260, 'min_child_samples': 85, 'min_split_gain': 0.05447428489222829, 'subsample': 0.30000000000000004, 'subsample_freq': 1, 'colsample_bytree': 0.7}. Best is trial 1 with value: 0.7814932479640063.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:42:23,903]\u001b[0m Trial 6 finished with value: 0.7661598907614501 and parameters: {'n_estimators': 500, 'learning_rate': 0.26338018769002486, 'num_leaves': 180, 'min_child_samples': 10, 'min_split_gain': 0.08899556540206363, 'subsample': 0.6000000000000001, 'subsample_freq': 1, 'colsample_bytree': 0.7}. Best is trial 1 with value: 0.7814932479640063.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:43:26,031]\u001b[0m Trial 7 finished with value: 0.7796901936854607 and parameters: {'n_estimators': 500, 'learning_rate': 0.06607396225425113, 'num_leaves': 20, 'min_child_samples': 85, 'min_split_gain': 0.02749908406524304, 'subsample': 0.5, 'subsample_freq': 1, 'colsample_bytree': 0.30000000000000004}. Best is trial 1 with value: 0.7814932479640063.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:44:32,706]\u001b[0m Trial 8 finished with value: 0.7814128095397013 and parameters: {'n_estimators': 500, 'learning_rate': 0.048548235345129656, 'num_leaves': 120, 'min_child_samples': 65, 'min_split_gain': 0.004043838307494019, 'subsample': 0.30000000000000004, 'subsample_freq': 1, 'colsample_bytree': 0.5}. Best is trial 1 with value: 0.7814932479640063.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:45:24,551]\u001b[0m Trial 9 finished with value: 0.7591914478058801 and parameters: {'n_estimators': 500, 'learning_rate': 0.2501296471868527, 'num_leaves': 280, 'min_child_samples': 90, 'min_split_gain': 0.09098493731409775, 'subsample': 0.4, 'subsample_freq': 1, 'colsample_bytree': 0.2}. Best is trial 1 with value: 0.7814932479640063.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:46:45,873]\u001b[0m Trial 10 finished with value: 0.7783575419503257 and parameters: {'n_estimators': 500, 'learning_rate': 0.1846520524344308, 'num_leaves': 100, 'min_child_samples': 100, 'min_split_gain': 0.05507499362508839, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.9}. Best is trial 1 with value: 0.7814932479640063.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:48:11,893]\u001b[0m Trial 11 finished with value: 0.7798186432799885 and parameters: {'n_estimators': 500, 'learning_rate': 0.1585533104062362, 'num_leaves': 100, 'min_child_samples': 70, 'min_split_gain': 0.07075094187851666, 'subsample': 0.9, 'subsample_freq': 1, 'colsample_bytree': 0.5}. Best is trial 1 with value: 0.7814932479640063.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:49:14,634]\u001b[0m Trial 12 finished with value: 0.7724386211641225 and parameters: {'n_estimators': 500, 'learning_rate': 0.07494320018972185, 'num_leaves': 120, 'min_child_samples': 35, 'min_split_gain': 0.0014700533953015127, 'subsample': 0.2, 'subsample_freq': 1, 'colsample_bytree': 0.5}. Best is trial 1 with value: 0.7814932479640063.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:50:40,930]\u001b[0m Trial 13 finished with value: 0.7829179119230725 and parameters: {'n_estimators': 500, 'learning_rate': 0.06118517673673758, 'num_leaves': 180, 'min_child_samples': 70, 'min_split_gain': 0.016679568868989296, 'subsample': 0.7, 'subsample_freq': 1, 'colsample_bytree': 0.4}. Best is trial 13 with value: 0.7829179119230725.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:51:56,819]\u001b[0m Trial 14 finished with value: 0.7805468312471919 and parameters: {'n_estimators': 500, 'learning_rate': 0.11238949420315447, 'num_leaves': 180, 'min_child_samples': 100, 'min_split_gain': 0.06971129275471684, 'subsample': 0.7, 'subsample_freq': 1, 'colsample_bytree': 0.4}. Best is trial 13 with value: 0.7829179119230725.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:53:27,211]\u001b[0m Trial 15 finished with value: 0.7721459614625428 and parameters: {'n_estimators': 500, 'learning_rate': 0.2002546033209316, 'num_leaves': 220, 'min_child_samples': 75, 'min_split_gain': 0.02488412879162292, 'subsample': 0.7, 'subsample_freq': 1, 'colsample_bytree': 0.4}. Best is trial 13 with value: 0.7829179119230725.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:54:52,952]\u001b[0m Trial 16 finished with value: 0.7813756515820653 and parameters: {'n_estimators': 500, 'learning_rate': 0.08672766946151235, 'num_leaves': 80, 'min_child_samples': 35, 'min_split_gain': 0.04072734697101215, 'subsample': 0.9, 'subsample_freq': 1, 'colsample_bytree': 0.4}. Best is trial 13 with value: 0.7829179119230725.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:56:23,299]\u001b[0m Trial 17 finished with value: 0.777471511612969 and parameters: {'n_estimators': 500, 'learning_rate': 0.13647457627249415, 'num_leaves': 220, 'min_child_samples': 90, 'min_split_gain': 0.015335312787580054, 'subsample': 0.6000000000000001, 'subsample_freq': 1, 'colsample_bytree': 0.6000000000000001}. Best is trial 13 with value: 0.7829179119230725.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:57:33,514]\u001b[0m Trial 18 finished with value: 0.770832470787131 and parameters: {'n_estimators': 500, 'learning_rate': 0.2992102176930833, 'num_leaves': 60, 'min_child_samples': 65, 'min_split_gain': 0.06705708181611837, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.30000000000000004}. Best is trial 13 with value: 0.7829179119230725.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 13:59:25,775]\u001b[0m Trial 19 finished with value: 0.7823456071136969 and parameters: {'n_estimators': 500, 'learning_rate': 0.05361381241615769, 'num_leaves': 140, 'min_child_samples': 10, 'min_split_gain': 0.037958265678033426, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.9}. Best is trial 13 with value: 0.7829179119230725.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:01:36,049]\u001b[0m Trial 20 finished with value: 0.781352173466511 and parameters: {'n_estimators': 500, 'learning_rate': 0.015087479185358042, 'num_leaves': 220, 'min_child_samples': 10, 'min_split_gain': 0.034819333295462176, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.9}. Best is trial 13 with value: 0.7829179119230725.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:03:03,091]\u001b[0m Trial 21 finished with value: 0.7813216958671818 and parameters: {'n_estimators': 500, 'learning_rate': 0.05109872629215857, 'num_leaves': 160, 'min_child_samples': 25, 'min_split_gain': 0.04660866542197741, 'subsample': 0.7, 'subsample_freq': 1, 'colsample_bytree': 0.6000000000000001}. Best is trial 13 with value: 0.7829179119230725.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:04:38,312]\u001b[0m Trial 22 finished with value: 0.7789355347358338 and parameters: {'n_estimators': 500, 'learning_rate': 0.09102680655965552, 'num_leaves': 140, 'min_child_samples': 25, 'min_split_gain': 0.013975885792637512, 'subsample': 0.6000000000000001, 'subsample_freq': 1, 'colsample_bytree': 0.8}. Best is trial 13 with value: 0.7829179119230725.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:06:02,564]\u001b[0m Trial 23 finished with value: 0.7839532786739699 and parameters: {'n_estimators': 500, 'learning_rate': 0.04598319176025238, 'num_leaves': 200, 'min_child_samples': 80, 'min_split_gain': 0.030668130202786536, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.4}. Best is trial 23 with value: 0.7839532786739699.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:07:39,546]\u001b[0m Trial 24 finished with value: 0.783169637675774 and parameters: {'n_estimators': 500, 'learning_rate': 0.0439944909805666, 'num_leaves': 200, 'min_child_samples': 45, 'min_split_gain': 0.03451531017847867, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.6000000000000001}. Best is trial 23 with value: 0.7839532786739699.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:09:21,433]\u001b[0m Trial 25 finished with value: 0.7834480237845444 and parameters: {'n_estimators': 500, 'learning_rate': 0.033214843717852036, 'num_leaves': 200, 'min_child_samples': 45, 'min_split_gain': 0.029786146211609556, 'subsample': 0.9, 'subsample_freq': 1, 'colsample_bytree': 0.6000000000000001}. Best is trial 23 with value: 0.7839532786739699.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:11:11,448]\u001b[0m Trial 26 finished with value: 0.7834656904495582 and parameters: {'n_estimators': 500, 'learning_rate': 0.042558459230342204, 'num_leaves': 200, 'min_child_samples': 45, 'min_split_gain': 0.030698851636132823, 'subsample': 0.9, 'subsample_freq': 1, 'colsample_bytree': 0.6000000000000001}. Best is trial 23 with value: 0.7839532786739699.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:13:01,019]\u001b[0m Trial 27 finished with value: 0.7837292553156532 and parameters: {'n_estimators': 500, 'learning_rate': 0.010442903169525234, 'num_leaves': 240, 'min_child_samples': 45, 'min_split_gain': 0.028955232620044794, 'subsample': 0.9, 'subsample_freq': 1, 'colsample_bytree': 0.6000000000000001}. Best is trial 23 with value: 0.7839532786739699.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:15:09,465]\u001b[0m Trial 28 finished with value: 0.78354454132789 and parameters: {'n_estimators': 500, 'learning_rate': 0.013455510595021583, 'num_leaves': 240, 'min_child_samples': 35, 'min_split_gain': 0.021960073991153985, 'subsample': 0.9, 'subsample_freq': 1, 'colsample_bytree': 0.8}. Best is trial 23 with value: 0.7839532786739699.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:17:20,629]\u001b[0m Trial 29 finished with value: 0.783382299172646 and parameters: {'n_estimators': 500, 'learning_rate': 0.0188748782712379, 'num_leaves': 240, 'min_child_samples': 35, 'min_split_gain': 0.019300149914774647, 'subsample': 0.9, 'subsample_freq': 1, 'colsample_bytree': 0.8}. Best is trial 23 with value: 0.7839532786739699.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:19:42,993]\u001b[0m Trial 30 finished with value: 0.7787158328149679 and parameters: {'n_estimators': 500, 'learning_rate': 0.13297353510344206, 'num_leaves': 240, 'min_child_samples': 25, 'min_split_gain': 0.00934282014286425, 'subsample': 0.9, 'subsample_freq': 1, 'colsample_bytree': 0.8}. Best is trial 23 with value: 0.7839532786739699.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:21:42,538]\u001b[0m Trial 31 finished with value: 0.784101603400559 and parameters: {'n_estimators': 500, 'learning_rate': 0.028841758172157103, 'num_leaves': 300, 'min_child_samples': 50, 'min_split_gain': 0.022503193364349777, 'subsample': 0.9, 'subsample_freq': 1, 'colsample_bytree': 0.5}. Best is trial 31 with value: 0.784101603400559.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:23:39,367]\u001b[0m Trial 32 finished with value: 0.7837466714889015 and parameters: {'n_estimators': 500, 'learning_rate': 0.01417342769115676, 'num_leaves': 300, 'min_child_samples': 50, 'min_split_gain': 0.022266265772549268, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.5}. Best is trial 31 with value: 0.784101603400559.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:25:23,846]\u001b[0m Trial 33 finished with value: 0.7845743698664226 and parameters: {'n_estimators': 500, 'learning_rate': 0.011188646759802946, 'num_leaves': 300, 'min_child_samples': 55, 'min_split_gain': 0.044362760312802674, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.5}. Best is trial 33 with value: 0.7845743698664226.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:27:11,468]\u001b[0m Trial 34 finished with value: 0.7842756660080963 and parameters: {'n_estimators': 500, 'learning_rate': 0.028664621142453994, 'num_leaves': 300, 'min_child_samples': 55, 'min_split_gain': 0.044203224330549386, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.5}. Best is trial 33 with value: 0.7845743698664226.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:28:58,985]\u001b[0m Trial 35 finished with value: 0.7850012151574163 and parameters: {'n_estimators': 500, 'learning_rate': 0.03307485015903331, 'num_leaves': 300, 'min_child_samples': 60, 'min_split_gain': 0.042603429782146755, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.5}. Best is trial 35 with value: 0.7850012151574163.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:30:32,187]\u001b[0m Trial 36 finished with value: 0.7845152091328809 and parameters: {'n_estimators': 500, 'learning_rate': 0.03293838097594997, 'num_leaves': 300, 'min_child_samples': 60, 'min_split_gain': 0.06164217070695967, 'subsample': 0.7, 'subsample_freq': 1, 'colsample_bytree': 0.5}. Best is trial 35 with value: 0.7850012151574163.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:32:11,270]\u001b[0m Trial 37 finished with value: 0.7815876015073433 and parameters: {'n_estimators': 500, 'learning_rate': 0.07714461080099755, 'num_leaves': 280, 'min_child_samples': 60, 'min_split_gain': 0.06219539932261141, 'subsample': 0.7, 'subsample_freq': 1, 'colsample_bytree': 0.5}. Best is trial 35 with value: 0.7850012151574163.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:33:47,081]\u001b[0m Trial 38 finished with value: 0.7837055957276702 and parameters: {'n_estimators': 500, 'learning_rate': 0.032010614280788094, 'num_leaves': 300, 'min_child_samples': 60, 'min_split_gain': 0.04520420819104808, 'subsample': 0.7, 'subsample_freq': 1, 'colsample_bytree': 0.4}. Best is trial 35 with value: 0.7850012151574163.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:35:16,318]\u001b[0m Trial 39 finished with value: 0.7823820235572984 and parameters: {'n_estimators': 500, 'learning_rate': 0.06277316214970546, 'num_leaves': 260, 'min_child_samples': 60, 'min_split_gain': 0.05565049176203965, 'subsample': 0.6000000000000001, 'subsample_freq': 1, 'colsample_bytree': 0.5}. Best is trial 35 with value: 0.7850012151574163.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:36:33,456]\u001b[0m Trial 40 finished with value: 0.7787256316916122 and parameters: {'n_estimators': 500, 'learning_rate': 0.10857064299072383, 'num_leaves': 280, 'min_child_samples': 65, 'min_split_gain': 0.07490601763783558, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.30000000000000004}. Best is trial 35 with value: 0.7850012151574163.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:38:24,920]\u001b[0m Trial 41 finished with value: 0.7846842193481658 and parameters: {'n_estimators': 500, 'learning_rate': 0.030911632893011097, 'num_leaves': 300, 'min_child_samples': 55, 'min_split_gain': 0.05040895271312271, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.5}. Best is trial 35 with value: 0.7850012151574163.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:40:12,915]\u001b[0m Trial 42 finished with value: 0.784020302076128 and parameters: {'n_estimators': 500, 'learning_rate': 0.03171515216424427, 'num_leaves': 300, 'min_child_samples': 55, 'min_split_gain': 0.0600102350182638, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.5}. Best is trial 35 with value: 0.7850012151574163.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:41:57,655]\u001b[0m Trial 43 finished with value: 0.7819540377809396 and parameters: {'n_estimators': 500, 'learning_rate': 0.06688538893540023, 'num_leaves': 280, 'min_child_samples': 55, 'min_split_gain': 0.0519778680505634, 'subsample': 0.7, 'subsample_freq': 1, 'colsample_bytree': 0.5}. Best is trial 35 with value: 0.7850012151574163.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:43:46,830]\u001b[0m Trial 44 finished with value: 0.7843723484473781 and parameters: {'n_estimators': 500, 'learning_rate': 0.030254039470650227, 'num_leaves': 260, 'min_child_samples': 50, 'min_split_gain': 0.042942193400515485, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.7}. Best is trial 35 with value: 0.7850012151574163.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:44:53,942]\u001b[0m Trial 45 finished with value: 0.7771863108490418 and parameters: {'n_estimators': 500, 'learning_rate': 0.09942050021261403, 'num_leaves': 260, 'min_child_samples': 50, 'min_split_gain': 0.07846853522825611, 'subsample': 0.4, 'subsample_freq': 1, 'colsample_bytree': 0.7}. Best is trial 35 with value: 0.7850012151574163.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:46:29,773]\u001b[0m Trial 46 finished with value: 0.7720431970976603 and parameters: {'n_estimators': 500, 'learning_rate': 0.20811085292659398, 'num_leaves': 280, 'min_child_samples': 70, 'min_split_gain': 0.05064035895044724, 'subsample': 0.7, 'subsample_freq': 1, 'colsample_bytree': 0.7}. Best is trial 35 with value: 0.7850012151574163.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:47:55,736]\u001b[0m Trial 47 finished with value: 0.7793085615145103 and parameters: {'n_estimators': 500, 'learning_rate': 0.07370153895785914, 'num_leaves': 260, 'min_child_samples': 40, 'min_split_gain': 0.062228126515522716, 'subsample': 0.5, 'subsample_freq': 1, 'colsample_bytree': 0.6000000000000001}. Best is trial 35 with value: 0.7850012151574163.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:49:24,508]\u001b[0m Trial 48 finished with value: 0.7837898856607199 and parameters: {'n_estimators': 500, 'learning_rate': 0.03879505667543741, 'num_leaves': 280, 'min_child_samples': 65, 'min_split_gain': 0.057654173060714665, 'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.4}. Best is trial 35 with value: 0.7850012151574163.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:50:48,766]\u001b[0m Trial 49 finished with value: 0.7864668370308898 and parameters: {'n_estimators': 500, 'learning_rate': 0.024659469629420172, 'num_leaves': 300, 'min_child_samples': 75, 'min_split_gain': 0.04075603156948057, 'subsample': 0.6000000000000001, 'subsample_freq': 1, 'colsample_bytree': 0.7}. Best is trial 49 with value: 0.7864668370308898.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:52:06,753]\u001b[0m Trial 50 finished with value: 0.782233550255581 and parameters: {'n_estimators': 500, 'learning_rate': 0.05626473480411026, 'num_leaves': 300, 'min_child_samples': 75, 'min_split_gain': 0.048911246745423, 'subsample': 0.6000000000000001, 'subsample_freq': 1, 'colsample_bytree': 0.4}. Best is trial 49 with value: 0.7864668370308898.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:53:42,659]\u001b[0m Trial 51 finished with value: 0.785749023358477 and parameters: {'n_estimators': 500, 'learning_rate': 0.02178172739325733, 'num_leaves': 260, 'min_child_samples': 60, 'min_split_gain': 0.03876209639668635, 'subsample': 0.7, 'subsample_freq': 1, 'colsample_bytree': 0.7}. Best is trial 49 with value: 0.7864668370308898.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:55:09,183]\u001b[0m Trial 52 finished with value: 0.7866870037122076 and parameters: {'n_estimators': 500, 'learning_rate': 0.02313014907219234, 'num_leaves': 280, 'min_child_samples': 80, 'min_split_gain': 0.038769636769923864, 'subsample': 0.7, 'subsample_freq': 1, 'colsample_bytree': 0.6000000000000001}. Best is trial 52 with value: 0.7866870037122076.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:56:33,515]\u001b[0m Trial 53 finished with value: 0.7875311650034603 and parameters: {'n_estimators': 500, 'learning_rate': 0.023424275362717226, 'num_leaves': 280, 'min_child_samples': 80, 'min_split_gain': 0.037983479292235105, 'subsample': 0.6000000000000001, 'subsample_freq': 1, 'colsample_bytree': 0.7}. Best is trial 53 with value: 0.7875311650034603.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:58:03,557]\u001b[0m Trial 54 finished with value: 0.786680641564711 and parameters: {'n_estimators': 500, 'learning_rate': 0.024615066154934803, 'num_leaves': 280, 'min_child_samples': 85, 'min_split_gain': 0.03749507723590403, 'subsample': 0.6000000000000001, 'subsample_freq': 1, 'colsample_bytree': 0.7}. Best is trial 53 with value: 0.7875311650034603.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 14:59:28,893]\u001b[0m Trial 55 finished with value: 0.787866771531478 and parameters: {'n_estimators': 500, 'learning_rate': 0.021652188029471605, 'num_leaves': 280, 'min_child_samples': 85, 'min_split_gain': 0.0393943242239961, 'subsample': 0.6000000000000001, 'subsample_freq': 1, 'colsample_bytree': 0.7}. Best is trial 55 with value: 0.787866771531478.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 15:01:19,130]\u001b[0m Trial 56 finished with value: 0.7877277673362373 and parameters: {'n_estimators': 500, 'learning_rate': 0.0209579955102583, 'num_leaves': 260, 'min_child_samples': 90, 'min_split_gain': 0.03727876731326529, 'subsample': 0.5, 'subsample_freq': 1, 'colsample_bytree': 0.7}. Best is trial 55 with value: 0.787866771531478.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-04-10 15:03:08,747]\u001b[0m Trial 57 finished with value: 0.7738892311589368 and parameters: {'n_estimators': 500, 'learning_rate': 0.16409008457231092, 'num_leaves': 280, 'min_child_samples': 95, 'min_split_gain': 0.03508520269966275, 'subsample': 0.5, 'subsample_freq': 1, 'colsample_bytree': 0.7}. Best is trial 55 with value: 0.787866771531478.\u001b[0m\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\distributions.py:548: UserWarning: The distribution is specified by [0.2, 0.95] and q=0.1, but the range is not divisible by `q`. It will be replaced by [0.2, 0.9].\n",
      "  low=low, old_high=old_high, high=high, step=q\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-3b37d23b0a52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"maximize\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudy_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"LGBM Classifier\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\study\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m             \u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m         )\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                 \u001b[0mprogress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m             )\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\testenv\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-120-e99a1ef9d91c>\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     37\u001b[0m         lgbm.fit(\n\u001b[0;32m     38\u001b[0m             \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0myt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mti\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         )\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    897\u001b[0m                     \u001b[0meval_init_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m                     \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m                     categorical_feature=categorical_feature, callbacks=callbacks, init_model=init_model)\n\u001b[0m\u001b[0;32m    900\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0minit_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m         )\n\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    290\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   3021\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   3022\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3023\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   3024\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3025\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\")\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"params = {'n_estimators': 500,\\n 'learning_rate': 0.021858316606688064,\\n 'num_leaves': 240,\\n 'min_child_samples': 65,\\n 'min_split_gain': 0.026936619524698613,\\n 'subsample': 0.9,\\n 'subsample_freq': 1,\\n 'colsample_bytree': 0.4}\""
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#params = study.best_params\n",
    "\n",
    "params = {'n_estimators': 500,\n",
    " 'learning_rate': 0.021652188029471605,\n",
    " 'num_leaves': 280,\n",
    " 'min_child_samples': 85,\n",
    " 'min_split_gain': 0.0393943242239961,\n",
    " 'subsample': 0.6000000000000001,\n",
    " 'subsample_freq': 1,\n",
    " 'colsample_bytree': 0.7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounding(ypred, por = [0.5, 0.5, 0.5, 0.5] ):\n",
    "    rez_round = np.zeros((len(ypred), ))\n",
    "    \n",
    "    rez_round[ypred < por[0]] = 0\n",
    "    \n",
    "    rez_round[ (ypred >= por[0]) & (ypred < 1 + por[1]) ] = 1\n",
    "    \n",
    "    rez_round[ (ypred >= 1 + por[1]) & (ypred < 2 + por[2]) ] = 2\n",
    "    \n",
    "    rez_round[ (ypred >= 2 + por[2]) & (ypred < 3 + por[3]) ] = 3\n",
    "    \n",
    "    rez_round[ ypred >= 3 + por[3] ] = 4\n",
    "    \n",
    "    return rez_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average several lgbm models with different random seed in order to eliminate dispersion of the predictions\n",
    "#helps not to reshuffle very much\n",
    "seeds = [154, 2277, 42, 5, 8713]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7853285234113403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787876743666825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.790653311005885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7896978564161007\n"
     ]
    }
   ],
   "source": [
    "#check quality on CV without rounding and save predictions for further rounding threshold adjustment\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y = df_r.y.values\n",
    "\n",
    "trues = []\n",
    "preds = []\n",
    "\n",
    "for ti, di in custom_cv(df_r):\n",
    "    Xt = X_TRAIN[ti]\n",
    "    yt = y[ti]\n",
    "\n",
    "\n",
    "    Xd = X_TRAIN[di]\n",
    "    yd = y[di]\n",
    "    \n",
    "    p_all = []\n",
    "    for seed in seeds:\n",
    "        lgbm = LGBMRegressor(objective = 'regression_l1', random_state=seed, **params)\n",
    "\n",
    "        lgbm.fit(Xt, yt, sample_weight = sample_weights[ti], categorical_feature=categorical_feature)\n",
    "\n",
    "        p_all.append(lgbm.predict(Xd))\n",
    "    \n",
    "    ypred = np.stack(p_all).mean(axis=0)\n",
    "\n",
    "    preds.append(ypred[::])\n",
    "    trues.append(yd[::])\n",
    "\n",
    "    #yr = rounding(ypred, [0.7, 0.5, 0.3, 0.4])\n",
    "\n",
    "    print(my_custom_loss_func(yd, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rounding thresholds search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f0408a19ed4587b401232feceff9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "pm = ()\n",
    "yrm = 0.0\n",
    "\n",
    "for p1 in tqdm(np.linspace(0.1, 0.9, 10)):\n",
    "    for p2 in np.linspace(0.1, 0.9, 10):\n",
    "        for p3 in np.linspace(0.1, 0.9, 10):\n",
    "            for p4 in np.linspace(0.1, 0.9, 10):\n",
    "                \n",
    "                rvv = []\n",
    "                for i in range(4):\n",
    "\n",
    "                    yr = rounding(preds[i], [p1, p2, p3, p4])\n",
    "                    \n",
    "                    rvv.append(my_custom_loss_func(trues[i], yr))\n",
    "                \n",
    "                rv = np.mean(rvv)\n",
    "                \n",
    "                if(rv > yrm + 0.0001):\n",
    "                    yrm = rv\n",
    "                    pm = [p1, p2, p3, p4]\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7222222222222222,\n",
       " 0.3666666666666667,\n",
       " 0.18888888888888888,\n",
       " 0.2777777777777778]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125892554340363"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8107731939435294"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check quality with found thresholds\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y = df_r.y.values\n",
    "\n",
    "cvr = []\n",
    "for ti, di in custom_cv(df_r):\n",
    "    Xt = X_TRAIN[ti]\n",
    "    yt = y[ti]\n",
    "    \n",
    "    \n",
    "    Xd = X_TRAIN[di]\n",
    "    yd = y[di]\n",
    "    \n",
    "    p_all = []\n",
    "    for seed in seeds:\n",
    "        lgbm = LGBMRegressor(objective = 'regression_l1', random_state=seed, **params)\n",
    "\n",
    "        lgbm.fit(Xt, yt, sample_weight = sample_weights[ti], categorical_feature=categorical_feature)\n",
    "\n",
    "        p_all.append(lgbm.predict(Xd))\n",
    "    \n",
    "    ypred = np.stack(p_all).mean(axis=0)\n",
    "    \n",
    "    yr = rounding(ypred, [0.7, 0.5, 0.2, 0.2])\n",
    "    \n",
    "    #print(yr)\n",
    "    \n",
    "    cvr.append(my_custom_loss_func2(yd, yr, di, df_r))\n",
    "    #cvr.append(my_custom_loss_func(yd, yr))\n",
    "    \n",
    "    #print(confusion_matrix(yd, [int(x) if x>=0 else 0 for x in np.round(ypred)]))\n",
    "    \n",
    "np.mean(cvr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets look on the predictions distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f24d50af08>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 477.85x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "df = pd.DataFrame(yd)\n",
    "df.columns = ['true']\n",
    "df['pred'] = ypred\n",
    "\n",
    "sns.displot(\n",
    "    data=df,\n",
    "    x=\"pred\", hue=\"true\",\n",
    "    kind=\"kde\", height=6,\n",
    "    multiple=\"fill\", clip=(0, None),\n",
    "    palette=\"ch:rot=-.25,hue=1,light=.75\",\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i in range(5):\n",
    "    plt.hist(df[df['true'] == i]['pred'], bins=100, alpha=0.5, label=str(i))\n",
    "plt.xlabel(\"Data\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n",
    "plt.title(\"Multiple Histograms with Matplotlib\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final prediction generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e = pd.read_csv(\"data_for_science_KT_evaluate.csv\", encoding='utf-8', sep=',')\n",
    "\n",
    "df_e[\"id\"] = np.arange(len(df_e))\n",
    "\n",
    "dfe = df_e.copy()\n",
    "dfe = dfe.sort_values(\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>DATE</th>\n",
       "      <th>KT_RESULT</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>644661</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>292115</td>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>556023</td>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>233104</td>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90502</td>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33075</th>\n",
       "      <td>1119183</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33076</th>\n",
       "      <td>473620</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33077</th>\n",
       "      <td>327819</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33078</th>\n",
       "      <td>452847</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33079</th>\n",
       "      <td>963363</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33080 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           idx        DATE  KT_RESULT     id\n",
       "0       644661  2020-04-17        NaN      0\n",
       "1       292115  2020-04-18        NaN      1\n",
       "2       556023  2020-04-18        NaN      2\n",
       "3       233104  2020-04-18        NaN      3\n",
       "4        90502  2020-04-18        NaN      4\n",
       "...        ...         ...        ...    ...\n",
       "33075  1119183  2020-07-11        NaN  33075\n",
       "33076   473620  2020-07-11        NaN  33076\n",
       "33077   327819  2020-07-11        NaN  33077\n",
       "33078   452847  2020-07-11        NaN  33078\n",
       "33079   963363  2020-07-11        NaN  33079\n",
       "\n",
       "[33080 rows x 4 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FeatureUnion] ......... (step 1 of 17) Processing year, total=   0.1s\n",
      "[FeatureUnion] ....... (step 2 of 17) Processing counts, total= 1.8min\n",
      "[FeatureUnion] ........ (step 3 of 17) Processing times, total= 2.1min\n",
      "[FeatureUnion] ....... (step 4 of 17) Processing gender, total=   0.1s\n",
      "[FeatureUnion] ..... (step 5 of 17) Processing antibody, total=  35.3s\n",
      "[FeatureUnion] .......... (step 6 of 17) Processing TTD, total=   0.1s\n",
      "[FeatureUnion] .... (step 7 of 17) Processing C-protein, total=  34.9s\n",
      "[FeatureUnion] . (step 8 of 17) Processing Simple Blood, total=  31.0s\n",
      "[FeatureUnion]  (step 9 of 17) Processing History Diases, total=   0.0s\n",
      "[FeatureUnion]  (step 10 of 17) Processing Respiratory Diases, total=  32.1s\n",
      "[FeatureUnion] ..... (step 11 of 17) Processing Receipe, total=  50.5s\n",
      "[FeatureUnion] ....... (step 12 of 17) Processing Mazok, total=  35.0s\n",
      "[FeatureUnion] ...... (step 13 of 17) Processing Reason, total= 1.1min\n",
      "[FeatureUnion] .... (step 14 of 17) Processing Question, total=   1.0s\n",
      "[FeatureUnion] ..... (step 15 of 17) Processing D-dimer, total=  26.7s\n",
      "[FeatureUnion] ...... (step 16 of 17) Processing Ferrum, total=  26.4s\n",
      "[FeatureUnion]  (step 17 of 17) Processing Antibody Stat, total=  34.0s\n",
      "[FeatureUnion]  (step 1 of 2) Processing Other Diases Near, total=  27.5s\n",
      "[FeatureUnion]  (step 2 of 2) Processing Resperatory Diases Near, total=  36.5s\n"
     ]
    }
   ],
   "source": [
    "X_train = union.fit_transform(df_r)\n",
    "X_train2 = union2.fit_transform(df_r)\n",
    "X_train = np.hstack([X_train, X_train2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = union.transform(dfe)\n",
    "X_test2 = union2.transform(dfe)\n",
    "X_test = np.hstack([X_test, X_test2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\hd898\\Anaconda3\\envs\\testenv\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    }
   ],
   "source": [
    "#lgbm = LGBMRegressor(objective = 'regression_l1', **params)\n",
    "#lgbm.fit(X_train, df_r.y.values, sample_weight = sample_weights, categorical_feature=categorical_feature)\n",
    "p_all = []\n",
    "for seed in seeds:\n",
    "    lgbm = LGBMRegressor(objective = 'regression_l1', random_state=seed, **params)\n",
    "\n",
    "    lgbm.fit(X_train, df_r.y.values, sample_weight = sample_weights, categorical_feature=categorical_feature)\n",
    "\n",
    "    p_all.append(lgbm.predict(X_test))\n",
    "    \n",
    "preds = np.stack(p_all).mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>DATE</th>\n",
       "      <th>KT_RESULT</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>644661</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>КТ-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9834</th>\n",
       "      <td>365812</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>КТ-2</td>\n",
       "      <td>9834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9833</th>\n",
       "      <td>435395</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>КТ-2</td>\n",
       "      <td>9833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>233104</td>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>КТ-3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9849</th>\n",
       "      <td>706150</td>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>КТ-2</td>\n",
       "      <td>9849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx        DATE KT_RESULT    id\n",
       "0     644661  2020-04-17      КТ-1     0\n",
       "9834  365812  2020-04-17      КТ-2  9834\n",
       "9833  435395  2020-04-17      КТ-2  9833\n",
       "3     233104  2020-04-18      КТ-3     3\n",
       "9849  706150  2020-04-18      КТ-2  9849"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfe[\"KT_RESULT\"] = ['КТ-' + str(int(x)) for x in rounding(preds, [0.7, 0.4, 0.2, 0.2])]\n",
    "dfe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "КТ-2    11092\n",
       "КТ-1     9386\n",
       "КТ-0     8864\n",
       "КТ-3     3530\n",
       "КТ-4      208\n",
       "Name: KT_RESULT, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfe.KT_RESULT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e = df_e.drop(columns=[\"KT_RESULT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e = df_e.merge(dfe[[\"id\", \"KT_RESULT\"]], on=\"id\", how='left')\n",
    "\n",
    "df_e = df_e.drop(columns=[\"id\"])\n",
    "\n",
    "df_e.to_csv(\"rez.csv\", encoding='utf-8', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>DATE</th>\n",
       "      <th>KT_RESULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>644661</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>КТ-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>292115</td>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>КТ-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>556023</td>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>КТ-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>233104</td>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>КТ-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90502</td>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>КТ-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33075</th>\n",
       "      <td>1119183</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>КТ-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33076</th>\n",
       "      <td>473620</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>КТ-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33077</th>\n",
       "      <td>327819</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>КТ-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33078</th>\n",
       "      <td>452847</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>КТ-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33079</th>\n",
       "      <td>963363</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>КТ-0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33080 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           idx        DATE KT_RESULT\n",
       "0       644661  2020-04-17      КТ-1\n",
       "1       292115  2020-04-18      КТ-2\n",
       "2       556023  2020-04-18      КТ-0\n",
       "3       233104  2020-04-18      КТ-3\n",
       "4        90502  2020-04-18      КТ-2\n",
       "...        ...         ...       ...\n",
       "33075  1119183  2020-07-11      КТ-0\n",
       "33076   473620  2020-07-11      КТ-1\n",
       "33077   327819  2020-07-11      КТ-2\n",
       "33078   452847  2020-07-11      КТ-1\n",
       "33079   963363  2020-07-11      КТ-0\n",
       "\n",
       "[33080 rows x 3 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
